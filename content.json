{"meta":{"title":"Hexo","subtitle":"","description":"","author":"John Doe","url":"http://example.com","root":"/"},"pages":[],"posts":[{"title":"machine_learning","slug":"machine-learning","date":"2025-04-02T17:29:12.000Z","updated":"2025-04-06T17:32:59.970Z","comments":true,"path":"2025/04/03/machine-learning/","permalink":"http://example.com/2025/04/03/machine-learning/","excerpt":"","text":"预测器，回归器或者自变量X中被加入了一列，这样就将截距式进行了转化。则作为结果，因变量。被称为误差项，噪声，这一部分代表了所有只影响y但与x无关的因素。估计的过程被称为从训练数据学习，目的有两个方面：（1）解释：理解y与x之间的关系；（2）预测：学习预测y。 逻辑回归考虑一个含n训练样本的数据集，由p个自变量组成，Y{0，1}是一个布尔符号： 我们假设任意维都与条件独立； 假设那么有logit的逆sigmoid 表示一组训练样本， 代表真实标签，正确估计的概率为： 那么所有样本估计正确的概率为：取log后得到log-likelihood：那么maximum likelihood就是寻找合适的 ,并且 分类任务在分类任务的语境下，我们一般用来表示。对应着的样本被称为正样本，反之则成为负样本。我们称为分类器，是在上的投影，因此 是表示了正负样本间差别的方向，因此应该与正样本同向，与负样本反向，也就是从负样本指向正样本。由之前的推导可以得到： 感知机感知机可以表示为,这里的w，b合在一起相当于： 三种模型生成模型生成模型是X与Y联合概率分布的统计模型，进行条件推理： 协方差矩阵X是一个n*p的矩阵，代表第k个样本的第i个特征 有以下性质: ，其中A是一个矩阵，b是一个向量 判别模型判别模型对Y在得到X条件下的条件概率建模，即，一般有softmax层的判别模型可表示为：其中表示经过k层神经网络后的输出，。 描述模型描述模型目标是描述的分布,可以表示为 Loss function为了训练得到，我们需要定义 最小二乘 线性回归相较于最小二乘回归，线性回归的值受到异常值影响更小。 逻辑回归根据之前的推导我们用negative loglikelihood作为损失函数而对于，的情况，则有：这个损失函数称为logistic loss。 用于分类的Loss函数 以上的损失函数都基于,我们称为间的margin。如果那么我们希望预测结果非常偏向正的，反之希望预测结果非常偏向负的。 最小二乘我们要求,可以对其求一阶导数，导数为0时有：在特殊情况下，如果X是正交归一的，这是可以简化为其中X是np的，是p1的，y是n*1的。对投影的推导,假设a在b上的投影为eb,e为一标量： 通过上面的推导，可以看到实际上是Y在X的列空间上的投影：残差表示了无法被当前模型表达的信息。 的分布假设存在一个真正的模型,则：假设,则可以求得： KL距离与交叉熵（略）最大似然辨别模型通过最大化下式来训练:由大数定律，在n足够大的情况下：前者为常数相对应的，生成或描述模型的最大似然则是通过对的估计。 梯度下降是学习率，是梯度，是某一步时的参数。 梯度下降算法：随机梯度下降：从n个样本中随机选择一个样本计算梯度Mini-batch:随机选择一个小批量，用其平均梯度作为梯度: 关于学习率：根据Robbins-Monroe theory我们通常要求：(1)(2)第一条保证算法可以达到最值，而第二条保证算法不会从最小值跳出。在实际情况中，我们会有更复杂的机制，比如在一定轮后减小学习率，在错误率稳定后缩小学习率等。 Momentum：包含有momentum机制的梯度下降,这一机制将之前的梯度作为后来梯度的一部分,下图中左半为随机梯度下降，右半为包含了momentum机制的随机梯度下降:Adagrad：不同参数的梯度可能是不同的，adagrad为每个参数自适应地调整学习率，用梯度除以梯度的平方和，可以让调整次数较少的参数更快学习，反之避免更新过快。是一个很小的数（比如）可以防止除0。RMSprop使用以下的机制：手动设置，因此,可以看到越靠前的梯度被递减越多。Adam优化器结合了RMSprop和momentum的机制： log-likelihood的梯度具有softmax层的判别模型:对于判别模型如果y=k(当y的真实标签是k)，则：这里的Y是独热矢量 batch_size并非越大越好在不考虑运算成本的情况下，将n个样本的梯度平均后学习不如将一定大小batch的样本梯度平均后学习。在小batch时，学习得到的梯度是有一定误差的，可以帮助跳出局部最小值。 langevinBrownian motion：其中如果将分为n段，,此时：在这种情况下：可以看到t足够小的情况下，速度趋近于无穷大。Langevin dynamic在训练过程中，则有我们需要手动改变的就是与。 LDA(Linear Discriminant Analysis)检验是否数据是线性可分的","categories":[],"tags":[]},{"title":"traffic_analysis","slug":"traffic-analysis","date":"2025-04-02T17:19:36.000Z","updated":"2025-04-02T17:19:36.461Z","comments":true,"path":"2025/04/03/traffic-analysis/","permalink":"http://example.com/2025/04/03/traffic-analysis/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2025-04-02T15:30:40.626Z","updated":"2025-04-02T15:30:40.626Z","comments":true,"path":"2025/04/02/hello-world/","permalink":"http://example.com/2025/04/02/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[],"tags":[]}